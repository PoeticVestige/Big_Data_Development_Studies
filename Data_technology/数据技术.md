# 数据技术 5期（ONE-ID介绍及应用场景）

![](.\images\5期_ONE-ID.png)

自己的理解：

- 对于基于ID关联的模糊匹配
  - 针对主应用（基本是本APP），然后根据手机号等，去找相同手机号的应用id，对它们进行关联
  - 有些id关联性较弱，可以对他进行去除
  - 做一些聚类等算法
- 对于基于图计算
  - 一个用户用设备id，手机号等进行关联某一应用（可以根据不同规则，形成一条“关联线”）
  - 可以有多个规则，可以关联上多个应用，形成“网络”

# 数据技术 6期（JSON解析）

见 SQL小点



# 数据技术 7期（用户标签画像体系）

## 用户标签介绍

### 什么是标签

用户标签反馈了用户基础属性、行为属性、生命周期、社交属性、行为偏好、营销偏好、用户体验、消费能力等等多个方面，可以为后续用户画像，用户分层，AB实验提供数据支持，一个用户可能身上会被打近百个标签（数据表字段）用于分析。

例子：

基础属性-基础信息：语兴今年28、性别男、青岛人

基础属性-身份信息：用户等级lv10

社交属性-关注属性：全网平台累计粉丝量2w+

偏好属性-内容偏好：对音乐内容感兴趣

用户体验-用户举报：最近30天被用户投诉0次

行为属性-发布属性：近30天发布课程3节

营销活动-活动参与：最近一次参与的活动为秋季大促

消费能力：客单均价在400元

通过如上标签可以粗略分析出用户具体样貌，可以继续后续数据加工。

### 标签属性

标签和指标的关系是包含的关系，标签可以直接为指标，标签也可以由指标组合形成，同时维度属性基础属性（维度和指标结合）也可以成为标签，因此我们常见标签属性包括如下几点：

- （1）文本信息：年龄、性别、手机号、ip、地区城市、设备、用户等级、vip等级
- （2）单枚举：是否高消费用户、是否活跃用户等都是0/1与Y/N组成
- （3）多枚举：年龄段（80后、90后）、用户xxx状态
- （4）复合指标/派生指标：最近30天最大消费金额、最近7天签到次数、最近30天离职率
- （5）派生日期：这里的日期不是单独的日期，而是日期加维度/周期，最近一次下单时间、第一次访问时间等

### 标签与指标

刚才语兴提到指标和标签关系，标签中枚举大多数会通过指标进行组合加工得到，同时指标也可以直接转化成标签去使用。

例如🍠社区业务中，是否最近7天是否社区活跃用户，如何评判最近7天用户是否活跃，可以通过如下几个指标去分析：

- （1）近7日连续登录次数大于3日
- （2）近7日发布作品大于2个
- （3）近7日点赞收藏转发作品次数大于15次
- （4）近7日评论次数大于10次
- （5）近7日参与社区活动次数大于2次

## 标签加工及开发

通常标签都是业务提出，例如业务方今天想看人群从而进行分析，会将要添加的标签口径找数分、数据产品对接，最后数仓完成标签开发，标签宽表设计有2种方式：

（1）各场景用户标签数据资产：这种方式则是将用户按照不同**属性（偏好、行为）**或者不同**产品内容（直播用户、短视频动态用户）**进行划分从而建设多个宽表，好处在于分类清晰，可按照数据表属性来取对应标签，坏处则在于如果业务方要看不同场景标签还需要去关联加工。 （见数仓建设实践路线，ADS建设）

（2）用户360标签数据资产：这种360全视角分析方式则是集中核心标签去分析用户，减少标签分散或者标签长期不用带来的检索问题，缺点也很明显，即标签属性只有核心属性。

总的来说就是两个大类，三个小类：

- 各场景用户标签数据资产（主题域）
  - 属性（偏好、行为）
  - 产品内容
- 用户360标签数据资产
  - 360

### 社区用户360背景

随着社区业务快速发展，与之相伴随的是社区核⼼用户资产的建设与沉淀，本项⽬以解决内容侧的⽤户标签缺失问题为⽬标，从多角度去分析用户数据，实现用户数据全貌展示，提升下游运营侧用户精准流量的定位，支持下游广告投放，营销活动等开展。

### 标签设计

数分拿到运营这个需求后需要想到的则是按照业务方用数视角去考虑，可以从业务环节出发、也可以从分析视角出发去拆解标签，我们作为数分按照短视频、直播、用户发帖、登录/评论/点赞/转发等行为可以联想到拆解出如下场景中标签，当然标签不止这些。

（1）用户基础属性：基础属性 用户属性:年龄分布，城市等级，蓝V，用户等级，系统类型，注册ip，这样可以用维度组合标签去观察。

（2）发布属性:最近30/60/90天账户创作，最近30/60/90账户写作评级，最近30/60/90平台曝光流推荐次数，近30天发布总量，最近90天最后一次动态发布时间

（3）生命周期: 用户广告停留时长，是否tab低活沉默(tab中近30天活跃小于等于3)，是否心智ugc，用户分层(低频用户 30天内tab访问低于2天 中频率)，是否推荐页内容点击用户，近30天连续登录天数

（4）社交属性:累计粉丝量，7天内创作者粉丝增长，粉丝量分层，近30天私信量

（5）行为偏好:第一偏好内容一级类目(社区浏览互动行为(点赞收藏 分享),加和排名第一类目)，评论次数，点赞次数，关注次数，收藏字数，搜索pv，喜欢发布帖子类型

（6）营销偏好:是否对科技数码兴趣 是否对健身兴趣，是否对穿搭兴趣

（7）用户体验:最近 30天被举报次数，到诱导关注，是否收到垃圾广告，被举报驳回次数，用户原始满意度 用户转化后满意度 用户是否收到收到骚扰

### 标签开发

数据仓库同学根据标签业务口径及标签属性内容，去下挖使用的数据表（直播信息表、内容发布表、用户关注表、用户行为操作表、举报记录表、内容搜索表、用户维度表等等），这里由于是360数据资产建设，所以我们只做一个ads数据宽表，由于我们分析的是用户360，因此需要以用户粒度为主，选择dim_user表为驱动表（主表）伪代码如下：

<img src=".\images\社区360指标开发伪代码.png" style="zoom:50%;" />

## 标签管理

当我们开发好标签宽表后可维护至标签画像平台中进行维护，方便后续进行人群全选，如果没有平台支持可以先通过共享excel文档去维护。

### 标签分组

标签分组和做指标域类似就是把之前定义好的基础信息、行为信息等按照一级二级类目进行划分，用Excel同学可以一个sheet页一个一级分组

### 标签新建

新建标签优先要绑定标签来源表，作为数据来源，这里如果没有平台同学只能通过手工记录数据来源来维护记录。

配置好数据源后可以在标签管理中进行标签维护，维护属性包括标签id、名称、颗粒度（重点）、标签类型（重点）、业务逻辑、sql逻辑、负责人、映射数据源的字段等等。

## 用户画像

### 用户画像生成

画像则是由多个用户标签组合生成，而标签则是由维度/指标组成的，维度和指标则是来源于用户数据，这就和最开始讲的为什么会被大数据杀熟，因为你在平台上做了操作就一定会被记录，就会被打标签。

画像一般则是业务通过平台进行全选，底层则是执行的where条件，例如语兴今天想筛出杭州地区最近30天发布动态数大于2且最近一次登录时间大于9月20的用户，sql如下:

```sql
SELECT user_id
FROM ycommunity.ads_user_360_profile
WHERE pt = '${bizdate}'
AND publish_trend_cnt_30d>2
AND substr(last_publish_time_30d,1,10)>='2024-09-20'
AND city='杭州'
```

如果没有平台的同学其实也能做，但是对于业务要求可能会定制化一些，因为很多时候业务看画像都是随机一些，而且大多业务不会sql，所以只能麻烦数仓同学一直开发画像表/字段，导致数仓不够灵活

### 平台生成用户画像

平台生成画像与之前讲的指标中心课程类似，都是拖拉拽就能生成想要的数据，但结束和上面sql查询出来的是一致，所以如果要考虑标签画像存放，建议先把数据同步到Star Rocks或者Doris中，再配置OLAP的数据源，再开发数据产品，需要考虑标签新建维护、画像拖拉拽功能生成等。



# 数据技术 8期（全量改增量）

[数据表全量改增量操作及拉链表设计 - 飞书云文档](https://my.feishu.cn/docx/J4V5d5J2eo7UdtxfaU8cbP5Hn8g)

## 之前全量表数据代码

数据抽取任务：

- 更改之前，是抽全部的数据到ods里存着。存到t-1的分区中，ods里每一天的分区存着的，都是全部数据

- 改造之后，主要是加过滤字段，将create_time（创建时间）或者modify_time（修改时间）为t-1的数据抽到ods中，ods里每一天的分区存着的，都是create_time（创建时间）或者modify_time（修改时间）为t-1的数据

全量没有用动态分区，所以，字段里没有pt（但查询的时候有pt这个字段）。而回刷分区、拉链表用到了动态分区，所以字段中有pt

```sql
-- 建表
CREATE TABLE dwd_trade_order_detail_df (
	...
    ,create_time STRING COMMENT '创建时间'
    ,modify_time STRING COMMENT '更新时间'
) COMMENT ''
PARTITIONED BY (`pt` STRING COMMENT '业务日期')
STORED AS PARQUET
TBLPROPERTIES('table.source'='自定义','table.creator'='yuxing','SYNC_METASTIRE'='on');

-- 插入
INSERT OVERRITE TABLE dwd_trade_order_detail_df PARTITION (pt='${bizdate}')
SELECT 
	   ...
	   ,t0.create_time -- 创建实践
	   ,t0.modify_time -- 更新时间
FROM ods_trade_order_detail_df t0 -- df是改之前的全量表
where t0.pt='${bizdate}' -- 因为ods的pt='${bizdate}'就是全量数据了
and is_del = 0;
```

## 全量改增量操作

都用得到动态分区，所以字段中会有pt这个字段

### 拉链表

做成拉链模式，限制分区为订单创建日期（pt=substr(create_time,1,10)），并通过订单状态去做两种状态记录，状态1为active（没完结的订单），状态2为history（历史订单），并生成2个拉链时间，zipper_start_time,zipper_end_time,如果是完成的历史订单则zipper_end_time为订单完结时间，并且统一存放到history分区，如果是没完结的订单则会当到active+pt分区中存放，并且生成拉链，这样坏处则是使用比较麻烦，很有可能下游不会用这张表导致扫描很长时间。

<img src=".\images\订单表_拉链表数据示例.png" style="zoom:50%;" />

#### 创建交易域-订单域-父订单明细表-拉链表

```sql
-- 建表
CREATE TABLE dwd_trade_order_detail_zipper_di(
	...
    ,create_time STRING COMMENT '订单创建时间'
    ,modify_time STRING COMMENT '订单修改时间'
    ,zipper_start_time STRING COMMENT '开链时间'
    ,zipper_end_time STRING COMMENT '关链时间'
    ，zipper_status COMMENT '拉链状态 activate 活跃 history 历史'
)COMMENT 'dwd-交易域-订单表-主订单-增量表'
PARTITIONED BY (`pt` STRING COMMENT '订单日期')
STORED AS PARQUET
TBLPROPERTIES('table.source'='自定义','table.creator'='nanshi','SYNC_MERASTORE'='on')
```

```sql
-- 拉链代码
-- 新增订单(当日新增) (若没数据，先跑新增数据，再跑拉链)
INSERT OVERWRITE TABLE dwd_trade_order_detail_zipper_di PARTITION (pt)
SELECT 
	   ...
	   ,t0.create_time STRING COMMENT '订单创建时间'
        ,t0.modify_time STRING COMMENT '订单修改时间'
        ,t0.zipper_start_time STRING COMMENT '开链时间'
        ,'2099-01-01' STRING COMMENT '关链时间'
        ,'activate' COMMENT '拉链状态 activate 活跃 history 历史'
        ,substr(create_time,1,10) AS pt
FROM ods_trade_order_detail_di t0
WHERE t0.pt = '${azkaban.flow.1.days.ago}'
```

```sql
-- 后继更变（一定不变的用t0，可能更变的用t1）
-- 此时已经有了历史数据
set hive.exec.dynamic.partition.mode=nonstrict;
INSERT OVERWRITE TABLE dwd_trade_order_detail_zipper_di PARTITION(pt)
SELECT 
	   ...
	   ,t0.XXX -- 这些都是老状态
	   ,t0.XXX 
	   ,t0.create_time -- 订单创建时间
       ,t0.modify_time -- 订单修改时间
       ,t0.zipper_start_time -- 开链时间
       ,least(t1.modify_time,lead(t1.zipper_start_time) OVER (PARTITION BY t0.order_id ORDER BY t0.zipper_start_time ASC)) AS zipper_end_time -- 关链时间
       ,IF(t1.order_status NOT IN ('5','6'),'active','history') AS zipper_status
       ,SUBSTR(t0.create_time,1,10) AS pt
FROM dwd_trade_order_detail_zipper_di t0
LEFT JOIN 
ods_trade_order_detail_di t1
ON t1.pt = '${azkaban.flow.1.days.ago}' -- ods的pt是业务日期
AND t0.order_id=t1.order_id
AND t1.is_del=0

WHERE t1.modify_time <> t1.create_time -- 当天新增的订单没进来（因为当天新增的订单，修改时间与创建时间一致），进来的都是过去已经有的订单，状态改变
AND t0.pt>='${azkaban.flow.60.days.ago}' -- 过去60天

UNION

SELECT 
	   ...
	   ,t1.XXX -- 这些都是老状态
	   ,t1.XXX 
	   ,t0.XXX -- 一定不变的用t0，可能更变的用t1
	   ,t0.create_time -- 订单创建时间
       ,t1.modify_time -- 订单修改时间
       ,t1.modify_time -- 开链时间
       ,'2099-01-01' -- '关链时间'
       ,IF(t1.order_status NOT IN ('5','6'),'active','history') AS zipper_status
       ,SUBSTR(t0.create_time,1,10) AS pt
FROM dwd_trade_order_detail_zipper_di t0
LEFT JOIN 
ods_trade_order_detail_di t1
ON t1.pt = '${azkaban.flow.1.days.ago}' -- ods的pt是业务日期
AND t0.order_id=t1.order_id
AND t1.is_del=0

WHERE t1.modify_time <> t1.create_time
AND t0.pt>='${azkaban.flow.60.days.ago}' -- 过去60天
```

![](.\images\全量改增量_拉链表模式_1.png)

![](.\images\全量改增量_拉链表模式_2.png)





### 回刷分区模式

回刷分区模式，限制分区pt=substr(create_time,1,10)，通过动态分区每日回刷近半年订单数据，好处在于直接使用pt分区即可，坏处在于看不到中间变化状态。

举个例子，比如t-3创建了一个订单，然后t-1状态改变了，直接刷了t-3分区里的数据，也就是t-3分区里的数据变了。然后t-1分区里存的是创建时间为t-1的数据。

注意 ods表是t1，它的字段里的pt是业务日期，不是订单创建日期

![](.\images\全量改增量_回刷分区模式.png)

# 数据技术 9期 （数仓分层）

[这次还是聊聊数仓分层（新思路） - 飞书云文档](https://my.feishu.cn/docx/ZkF5dRInkoJD9KxYSXMcRBxQnXc)

![](.\images\9期_数仓分层_1.png)

- Ods ：接入层，可拆解为ori 板块和ods板块，都可以叫ods，也可以分开起名，ori主要做json等消息存储，ods则是存放解析json成各类字段后的数据，如从mysql等数据库抽取的数据则字段与数据源保持一致，数据类型整形使用bigint string double/decimal等
  - 命名：ods\_业务库名\_表名\_df/ods_topic_df
- Dwd 
  - 明细层，主要做数据清洗（test、异常等数据），转换（将code值转化成name），关联（按照相同数据域进行关联组合），同时采用模型5要素（数据域、维度（可不做到dwd 不一定需要维度退化）、度量、颗粒度（买家id 卖家id）、事实）
  - Di/df需要考量你的数据量级，分区（可按照业务进行二级分区划分），以及数据本身形态（例如日志数据每天不一样的数据情况）
  - 枚举值，可以手动维护到一个Excel表里，每次枚举值更新，都记录到Excel表里
  - 数据域：按照业务流程拆解，如没有业务流程可按照产品分类 业务类型分，例如招聘这个大业务面试（一级域）拆解成 投递、面试评价 面试流程 面试结果、背调、offer
  - 这里有同学会想到为啥这里不退化维度，1同数据域维度重复2刷新维度代价高(放到DWM做)
  - 命名：dwd\_{一级数据域}\_{二级数据域}\_xx_detail_df/di
- Dwm层：明细宽表层，通过业务流程起始id进行关联其他表，例如应聘表的应聘id关联面试明细、面试结果表，直到关联到最后业务背调，这里我们进行维度退化（就是事实表left join 维度表 把右表维度字段带入到dwm），把维度带入明细宽表中更方便维护，同时也可以给业务方提供明细宽表方便查数，包括后续做Dws ads都可以使用dwm表，数据域保持一致。
  - 命名：dwm\_一级域\_xx\_detail\_df/di
- Dws ：轻度汇总层，首先统一指标口径先确定指标，其次按照周期（最近30 60 90天），颗粒度（拆到不能再拆）完成指标聚合。暂时不需要维度。可以和DWM的域保持一致

目前语兴新的建模办法：dws 按照周期加颗粒度汇总，举例dws 交易域最近30天用户指标，where datediff (t-1，create time)<=30 group by user id就行，去做派生指标，**ads加工时候用维度表作主表（dim user）关联我dws做好的表就行**，只汇总一次，最后全部做成标签宽表，下游分析使用时候只需要在取数平台 或者可视化平台拉拽维度和标签就行，也可以用于人群圈选，1举2得

命名·：DWS\_一级域\_二级域(可去掉)\_聚合的颗粒度\_xx_target\_30d

- Ads：

  - 大宽表：根据业务划分主题域（例如员工管理），更多是围绕业务分析视角去分析，大宽表则建议做成标签表，可以是不同**维度**作为**颗粒度**的标签表，也可以是用户标签表，再或者主题域与数据域再一致去做标签，举例子员工招聘标签表，以员工维度为主表left join dws_xxx left join (select empid ,romnumber over(partition by emp id order by)) on empid=empid
    去做标签
    这里一定要**确定颗粒度**， 可关联dwd dws形成标签表，所以大家会发现这里我直接用empid关联empid因为我dws 只group by一个颗粒度字段保持唯一且好用，同时大宽表可以为很多专项提供服务（例如不同看板）
  - 小宽表：小宽表的意义更多在于单独去做某个专题，例如员工离职动因分析，leader分析等，很多标签都要复用大宽表，基于大宽表再去定制开发，所以大宽表要做的更通用一些
  - 命名：ads\_一级主题域\_二级主题域（可去掉）\_颗粒度\_xx\_d

  

# 数据技术 10期 （数据开发流程常出现的数据质量问题）

## 问题背景

最近语兴从同事视角以及自身视角发现我们部门其中大多数问题集中在数据质量，让业务失去数据信心，虽然我们有开发流程，前期也会和业务对接，但最后业务方还是能发现好几个数据BUG,反复赶工，因此语兴复盘了一下导致数据质量问题发生的情况。

（1）没有数据产品经理对接业务。

（2）口径一直对不齐，没有明确口径

（3）测试前指标看上去没有问题，但上线后出现问题很多（没搞清楚底层数据情况）

（4）因为需求过多，历史bug工单太多导致没时间仔细code review。

（5）源头数据质量就有问题，但业务方总以为是数仓问题。

## 问题拆解

- **没有数据产品经理对接业务**：正常开发流程中都是需要数据产品/数据分析来对接业务的，好处在于

  - 能帮数据开发省去很多对接业务时间以及开会时间，很多无用的会数据开发可不参与，由数据产品直接去对接，从而留下较多时间投入开发。
  - 能比数据开发更懂业务流程，作为数据开发如果需要懂业务流程仍然需要对接业务及对接后端搞清楚数据表才可，因此一个需求进度就会很长，哪怕是加字段需求都需要了解全部项目背景，很费心（当然如果有充足时间还是可以了解下，但在互联网数仓中没有太多时间去了解这么全面）
  - 没有uat测试（uat测试是指后段系统上线后产品再去测试看看整体系统是否有问题），因此数仓也需要uat，需要从开发角度看到更细节的问题，如果直接对接业务就会出现很多bug，这时数据产品用处就很大了，能提前拦截不必要bug，提升业务用数信心。

- **口径一直对不齐，没有明确口径**：口径对不齐也是数据质量常出问题的情况，可能是一开始对接需求时，业务方没讲清，最后交付数据出现问题，也可能是对接需求中业务频繁改，业务也不知道他最后到底看哪个口径，或者说业务也不清楚到底要分析哪些指标，对于这种情况一定要让业务方先想好，同时让数据产品拍定最后方案（这里又强调了数据产品重要性），如后续更改需要走下期迭代，这也就是语兴常在简历中提到的统一指标口径的关键。

- **测试前指标看上去没有问题，但上线后出现问题很多（没搞清楚底层数据情况）**：这个问题很多数据开发同学都会遇到，明明在上线发布之前指标验证都是正确的，为什么过了一段时间数据又不对了，对于这种只能说明细数据还有一些细节没搞清楚，例如一些type值，或者要去除的数据没去除导致指标不对，这种情况还是要熟悉明细数据，只到问清楚后端所有枚举信息，并与业务确认要包含哪些枚举，不需要哪些数据，在开发指标前把明细数据理清，同时在开发好后观察指标1-2天（如果是不着急交付）

- **因为需求过多，历史bug工单太多导致没时间仔细code review**：我敢肯定目前80%以上公司都没有完整的数据上线前规范，很多同学都是面向业务去开发，更不用提数仓分层穿透这些问题，本质在于没有时间开发，对！本质就在于没有时间去做需求，甚至平时还要处理历史线上bug，去开一些无用的会，因此草草了事发布被业务发现问题也是常出现的情况，我们先讲一下完整的数据仓库上线发布前完整流程：

  - （1）代码是否能跑通：首先需要在开发环境先去跑通代码，确保代码是是没问题的。
  - （2）数据探查：上线之前先进行数据探查可以通过工具去看，也可以通过sql查询去看数据分布看最大值、最小值、空值、最大字符串长度、去重前后行数等等。
  - （3）数据比对：如果是加字段需求，除了需要去探查指标，其次还需要去看一下加了这个字段后对之前指标的影响，比如加了这个字段导致数据膨胀，因此需要再做一下数据比对，可以先把数据跑到开发环境的库中，例如现在开发环境是101个字段，线上是100个字段，加了这一个字段后，可以用开发环境的表和线上的表去比对，看一下之前的数据能否一致，这里有工具是最好的能直接检验，如果没工具可以抽几百条数据去看一下之前100个字段的数据是否一致，如不一致需要看一下具体情况。
  - （4）抽样比对：通过的ods抽样数据和ads数据去比对看看是否一致，例如今天做了一个指标可以写一段sql从ods查这个指标再和我们开发好的ads比对，可以抽200条user_id去看是否一致，但这里数据只能保障200条是准的，不能保障100%指标是准确的，但我们也只能检测到这个情况。
  - （6）依赖检测：通过的ods抽样数据和ads数据去比对看看是否一致，例如今天做了一个指标可以写一段sql从ods查这个指标再和我们开发好的ads比对，可以抽200条user_id去看是否一致，但这里数据只能保障200条是准的，不能保障100%指标是准确的，但我们也只能检测到这个情况。
  - （7）上线后回补数据：如果是全量数据则跑t-1即可，如果是增量数据需要进行补数据，为了检测生产环境是否有访问表的权限，如果没有的话生产环境运行代码也会报错，同时也是为了刷新数据后续交付。
  - （8）数据UAT测试：上线发布后需要让数据产品接入，再去看一下指标是否有异常（空值、枚举值分布、一场值等）
  - （9）DQC/基线SLA配置：配置监控告警，防止每日问题数据向下游流出，保障每日任务正常运行。

  这是完整的链路，语兴相信大家能完全遵守每一条是不太现实的，但还是需要自测一下，重点需要关注数据探查、抽样比对、依赖检查、上线回补数据、DQC/SLA配置，其实做到这些点已经能规避60%问题出现，剩下情况就需要看需求是否堆积，以及是否有数据分析/数据产品。

- 源头数据质量就有问题，但业务方总以为是数仓问题。

  - 其实这个问题跟数仓无关，但总是被业务扣帽子，就说你数据质量问题，但作为数据开发同学心里也苦，明明不是我的锅，但也没办法，既然出现了就要澄清及解决
  - （1）需要让后端出面解释，可以拉数据质量沟通群中，做出源头数据解释，同时数仓也需要截图证明ods接入的数据确实有问题，并发群里。
  - （2）见数仓建设实践路线的数据质量长期监测跟踪体系

# 数据技术 15期 （ID生成）

## ID生成

- 订单ID的要求
  - 全局唯一性
  - 高性能：并发要高、延迟低、在一些大型公司，很重要
  - 高可用：稳定、不容易挂掉
  - 简单易用
- 目前ID生成有以下几种方法
  - 自增ID：一般对于要求不高，量级和并发都不大的情况下使用
  - 时间戳+随机数
  - 分布式算法
    - UUID生成
    - 雪花算法
  - 订单序号+其他信息（时间、日期、序号、...）
    - 淘宝id：就是序号+买方信息等拼接而成

## ID相关

- 订单id生成- 案例1
  - 订单ID：一般以Long类型数据存在并且全局唯一。在用户下单过程中一般根据下单的时间戳拼接而成，如下为一种生成逻辑
    ![](.\images\ID生成案例1.png)

- 订单id生成- 案例2
  - 订单号的生成跟订单id生成类型差不多，但是底层逻辑上有所不同
    <img src=".\images\ID生成案例2.png" style="zoom:67%;" />

总之就是把long类型数据的64个bit位拆开，不同部分，记录不同信息



## 数据中如何使用

- 由于订单时整形，可以利用bitmap的方式对订单进行去重
- 由于订单中包含秒级的完整时间，就可以不需要全局排序获取用户首末次订单和对应的时间信息
- 可以利用订单id生成方式，重新拼接信息，来进行更多信息编码
  - 用户id，下单时间，下单场景，下单店铺，下单场景
  - 用UDF解析id，就可以获取对应的信息



# 数据技术 17期 （大模型与数据开发结合）

## 工作流

- agent解释：ai agent也叫人工智能代理，是一种能够感知环境、进行决策和执行动作的智能实体。智能体像人一样，它有记忆、有逻辑分析能力、有任务拆解能力、问题的拆解能力和最后综合回来统一解决问题的能力，例如自动回复邮件的程序，自动驾驶都叫agent。
- dify：是一个集成开发环境，提供低代码工具帮助开发者构建、部署和管理AI应用，如聊天机器人、智能助手等。

### dify平台功能

#### 探索

可查看和使用定义好的Chat bot、chatflow、workflow、agent

#### 工作室

可查看、编辑和创建Chatbot、chatflow、workflow、agent

#### 知识库

可查看、维护知识库（知识库可作为上下文被其他智能体中的大模型引用），目前仅自己创建可用

### workflow工作流

基于workflow工作流可以获取大模型执行的信息，同时还会根据每个节点执行情况，节点包括（input输入数据，output输出数据，预处理（python对数据二次加工），调用大模型，条件分支等），可以将节点看作离线任务链路中加工的节点，最后根据大模型对数据识别后将识别的数据output输出到大模型工作流表中。

![](.\images\17期_大模型与数据结合_1.png)

## 数据与大模型配合类型

- 偏业务
  谈到大模型+数仓组合，通常大家都会想到通过开发底层数据完成大模型训练，从而提供数据支撑，这个观点是对的，通常大模型+数仓组合一般呈现为2步，即
  - 清洗数据保障数据质量后将明细投放到大模型中（清洗数据）。
  - 通过大模型的工作流执行及识别精准程度进行数据分析（对数据打标）。
- 偏数仓自用

### 数据前置生成投入大模型进行达标（偏业务）

数据前置生成投入即清洗好原始宽表数据信息，选择更贴合场景的内容数据，例如今天我们要做模拟抖音进行内容生成，那需要提前爬取抖音优质/劣质内容（具体看你用在什么场景，风控就用擦边/涉政类，优质就爬优质的），接入数仓ODS->DWD->ADS（ODS接入原始逆向也叫爬虫的数据，DWD进行json解析即数据清洗维度下沉，ADS进行内容打标（因为走实时，没有做DWS，直接从DWD到ADS），标记内容优质情况，当然打标可以走算法模型，最终封装json，to Kafka，推送大模型平台，当input）

 内容打标后，输入到大模型中的数据得视情况而定，若是想让其生成好的内容，则输入数据，也得是好的，反之为坏的数据。

文本数据contxt，图片、视频url，组成List。

<img src=".\images\17期_大模型与数据结合_2.png" style="zoom:67%;" />

### 大模型后置产出数据进行测算估计

当经过workflow工作流识别、产出后的数据可以用于评估业务召唤/准确率，结合各种维度属性进行大模型生成内容调整，从而调整工作流，因此数据开发在大模型后置则是基于workflow返回数据及业务数据构建ods到ads链路，建设指标搭建报表。

结果可以生成好几张表：

- 工作流的表，输入、产出的数据表
- 评估投放效果表
- 节点状态表（有点类似元数据），就是记录节点消耗的token、失败的次数，可以做个报表

<img src=".\images\17期_大模型与数据结合_3.png" style="zoom:67%;" />

完整链路图

![](.\images\17期_大模型与数据结合_4.png)

数分：大模型后置的指标制定，工作流的建设

数开：大模型前置的数据清洗，数据投喂，大模型后置的报表的ODS到ADS的建设。

大模型产品：根据不同场景做Agent ，调研可能要用到的数据，做工作流。本质是提效。

大模型研发：就是做类似dify的这样的平台，拖拉拽，被调用记录，配置各种数据源，调参数

### 大模型再数仓中的使用

大模型在数仓中投入可分为几个方向，这里别说什么网易chat bi、SQL copilot这种，已经不是数仓方向能干出来的，我们就以chat bi为例，

（1）企业内部chatbi难落地:依赖数据开发、数据产品、数据平台等各方配合，开发周期长，人力投入，还需要长期调整精度，roi较低（正儿八经使用的人甚至不如取数据平台），平台价值远不如做业务价值，同时也很考验数据开发的资产完善情况，当前大部分数仓表较为分散，资产烟囱式建设太多。

（2）购买三方平台：购买3方（quick bi，网易有数较为成熟），价格太贵，带来的收益也低，即使有用户在用也很难用明白（最后变成数据表查询智能问答工具，还不如做个agent），如果只是为了套壳满足一些混子高层汇报吹牛逼那还是够的。

因此，我们只说数仓一个组能做的事

（1）根据数据表元数据，数据资产自动化评估有效资产、核心资产、是否可下线及优化等，并可以做一个简单问答助手。

（2）根据任务执行元数据（消耗cpu、内存、近15日执行时间、执行sql语句等）识别不良任务进行自动化推送诊断，结合血缘对于无效任务进行推荐冻结/下线









# 数据技术 18期 （调度配置的使用）

![](.\images\18期_调度配置使用.png)

- 实例：由上线的任务，具体数据进来后，由代码生成的实际任务案例
- 空跑是正常调度，但不执行任务的代码
- 合理分配基线
- 调度任务名称、与表要规范化





# 数据技术 19期 （宽表与窄表）

宽表：字段特别多

窄表：把宽表的列转成行。（有点炸裂函数的感觉）

## 使用问题

- 宽表：开发的时候遇到定制化，业务方要求一直加字段，加字段，然后一直膨胀，顶不住
- 窄表：行特别多，想获取不同字段，有点麻烦，不好用

## 使用场景

- 宽表：画像标签，给分析用的，标签足够多，不太定制化
- 窄表：推数的时候使用，比如给后端、算法推数据的时候可以使用。
  - 比如风控客诉中，有一个字段名字叫，平台负面信息，如果做成宽表，在很多时候，这个字段一直是空着的没用，这时候，做成窄表的化，就没有这个问题。
  - 可以做二级分区，日期为一级分区，规则为二级分区，每次查询的时候，都可以直接定位到规则

<img src=".\images\19期_风控客诉_宽表与窄表_2.png" style="zoom:80%;" />

<img src=".\images\19期_风控客诉_宽表与窄表.png" style="zoom:50%;" />











































































