#  第1期 小文件治理

## 一、背景

### 小文件是如何产生的

1.日常任务及动态分区插入数据（使用Spark2 MapReduce引擎），产生大量的小文件，从而导致Map数量剧增；

2.reduce数量越多，小文件也越多（Reduce的个数和输出文件是对应的）

3.数据源本身就包含大量小文件，Api、Kafka等。（这类型的数据源过来时，要先做个trans，做Json解析、做小文件的一个合并）

4.实时数据落Hive也会产生大量小文件

### 小文件的影响

1.从Hive的角度看，小文件会开很多Map，一个Map开一个JVM去执行，所以这些任务的初始化，启动，执行会兰妃大量的资源，严重影响性能。

2.在HDFS中，每个小文件对象约占150Byte，如果小文件过多会占用大量内存，会直接影响NameNode性能，相对的如果HDFS读写小文件也会更加耗时，因为每次都需要从NameNode获取元信息，并与对应的DataNode建立连接，如果NameNode在宕机中回复，也需要更多的时间从元数据文件中加载；

3.占据HDFS存储，从下图我们得知21号文件平均后存储为280K，合并后为249K； 

## 二、小文件的解决方案

### 1、计算引擎使用Spark3合并小文件

Spark能够通过AQE特性自动合并较小的分区，对于动态分区写入Spark3.2+引入了Rebalance操作，借助于AQE来平衡分区，进行校分区合并和倾斜分区拆分，避免分区数据过大或过小，能够很好处理小文件问题。

AQE解释：

Spark 社区在 DAG Scheduler 中，新增了一个 API 在支持提交单个 Map 阶段，以及在运行时修改 shuffle 分区数等等，而这些就是 AQE（分区合并），在 Spark 运行时，每当一个 Shuffle、Map 阶段进行完毕，AQE就会统计这个阶段的信息，并且基于规则进行动态调整并修正还未执行的任务逻辑计算与物理计划（在条件运行的情况下），使得 Spark 程序在接下来的运行过程中得到优化。

### 2、减少reduce的数量

计算引擎为Hive，可以使用参数进行控制：

```sql
SET mapred.reduce.tasks=100;
SET mapred.reduce.tasks=100;
INSERT OVERWRITE TABLE xxx.xxx PARTITION(ds='lst1date')
```

### 3、Distribute By Rand()

Distribute by rand()控制分区中数据量，使得Spark SQL的执行计划中多一个Shuffle，用于代码结尾（Distribute by ：用来控制Map输出结果的分发，即Map端如何拆分数据给Reduce端。 会根据Distribute by 后边定义的列，根据Reduce的个数进行数据分发，默认是采用hash算法。当 Distribute by 后边跟的列是Rand()时，即保证每个分区的数据量基本一致）

### 4、在数据传输任务后再做一个清洗任务

本质也是回刷分区合并小文件，去处理小文件保障从数据源开始小文件不向下游流去。

<img src=".\images\小文件治理_1.png" style="zoom:50%;" />

### 5、实时任务传输hive后采用每天调度任务来合并小文件

![](.\images\小文件治理_2.png)

这是Kafka to Hive 任务，在有了hive表之后，每日调度任务，来合并小文件。

```sql
SET hive.exec.dynamic.partition.mode=nonstrict; -- 开启动态分区
SET spark.sql.hive.ocnvertInsertingPartitionedTable=false; -- 让Spark3 跑出来的结果可以导入到Impala
SET spark.sql.optimizer.insertRepartitionBeforeWriteIfNoShuffle.enabled=true; -- 把shuffle关掉

INSERT OVERWRITE TABLE xxx.ods_kafka_xxxx PARTITION (ds)
SELECT id
	   ,xxx_date
	   ,xxx_type
	   ,ds
FROM xxx._ods_kafka_xxxx
WHERE ds = '${kst1date}' -- t-1 参数
```

### 6、通过参数方式合并小文件

```sql
-- Hive
SET hive.merge.mapfiles=true; -- 默认值ture,在Map-only的任务结束时合并小文件。
SET hive.merge.mapredfiles=true; --  默认值false,在Map-Reduce的任务结束时合并小文件，比较少用。
SET hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat; -- 执行MAP前进行小文件合并

-- Spark2：
SET spark.sql.finalStage.adaptive.adaptive.advisor.advisoryPartitionSizenInBytes=2048M; -- 可直接用
```

## 三、对已有的小文件处理

### 1.使用Spark3进行动态分区刷新

代码如下：

![](.\images\小文件治理_3.png)



### 2.重建表

这里如果是无分区的表可以考虑直接将表删掉，再重建，使用Spark3跑数据

## 四、小文件治理问题点

-  问题1：

用spark3+动态分区合并小文件发现一个问题，如果我给分区固定日期 小文件原30个会合并1个，如果用动态分区的话刷完可能部分分区还是30个，后面问了数据平台大佬，大佬说没加

`spark.sql.optimizer.insertRepartitionBeforeWriteIfNoShuffle.enabled=true`

，原理是把inset select from这种简单的没shffule的合并小文件关掉的，动态分区写入和静态分区写入时候创建文件的姿势确实是不一样的

- 问题2：

实时数据传入hive小文件居多也需要合并，这里我们可以把历史数据通过spark3+动态分区先回刷 后续建一个每日spark3的调度任务刷t-1的小文件即可

- 问题3：

使用Spark3刷小文件时候如果用到Impala同学记住一定要加这个参数

`SET spark.sql.hive.convertInsertingPartitionedTable=false; `解决Spark3刷新数据后无法同步到Imapla

## 五、小工具治理工具化与治理效果

![](.\images\小文件治理_4.png)



# 第2期 数仓开发流程讲解

见数仓建设学习路线

## 问题1：需求紧

业务要求：两天必须产出报表，实际分层开发需要五天，如何解决？

答：0.5天了解业务，1天写代码建模型，直接从ods到ads产出数据并且这里先做ads，（可以先给个空表）支持bi看板搭建同时进行，等看板完事——模型开发完，0.5天数据测试。这并不代表完全结束，在完成需求后，需要根据ads做dws，dwd，或者重新从ods开始做。

BI和风控同时提出需求，都非常急，怎么办？

 答：找到他们的leader，各做核心的一半，（比如把需求做一半），看看能不能接受，或者一方能不能推迟一点儿。

## 问题2： 换业务，快速融入

如果你从之前的行业换到现在这个行业，不用的业务，你是如何快速学习融入的？

答：先看报表核心指标，结合prd文档，不断拆解到原子指标来了解业务过程要做什么。

没有文档，看代码，倒推指标，看数据域，是快速了解业务流程的方式。

可以看数仓技术学习路线.xmind

## 问题3：0-1搭建数仓

答：见数仓建设学习路线-数据应用

数仓建设周期

- 初始期（在这个问题，主要回答这个部分）
  - 选架构（离线场景多还是实时场景多），自己搭，选平台？
  - 面向分析，划分主题域，根据主题域看看有哪些业务，再细分数据域
  - 看看可以做哪些数据表，数据量大概有多少，做评估 。
  - 画数仓版图（梳理上下游，数据从哪儿来）
    - 数据应用
    - 数据服务
    - 数据资产
    - 数据基建
  - 标准的定制。找上述的对接人，该配置的数据源、开发环境弄好
  - 核心数据同步，开始做：
    - 了解业务、划分数据域
    - 构建总线矩阵
    - ...
  - 支持看板等（数据应用）
- 扩张期
- 治理期（缓慢发展期）
- 变革期



# 第3期 数据表（数据模型）资源合规治理

算是数仓建设实践路线-数据治理部分的补充内容

 数据标准重制定->无用/临时数据表下线->应用指标公共下沉复用->解决ODS穿透问题->烟囱数据表重构及下线->元数据非合规数据表（包括元数据字段信息）修改（改字段）。

下游一般用的是ADS表，只要ADS不出问题，能产出，DWD，DWS等可以随便捣鼓，目的是为了辅助产出ADS。

## 1.数据标准重新制定

对当前数据域/主题域按照业务流程及应用重新划分,重新制定元数据规范(表名、字
段命名、字段数据格式等)用于新数据表建设中规范内容。
(1)数据域与主题域区别及如何划分

- 主题域:从业务视角自上而下分析,从整体业务环节中升华出来大的专项分析模块,结合对接的业务范围和行业形态从更高的视角去洞察整个业务流程;
- 数据域:从数据视角自下而上搭建,对每个业务环节进行切割划分,形成不同环节的数据集,组装为完整的业务流程;

可能大家看到这还是有点懵,那在这里举个例子:假如现在的业务是金融产品(例如贷款、赊购等),按照金融产品业务生命周期拆解,可以分为贷前(准入、授信等)、贷中(支用、还款等)、贷后(催回等),那这里贷前中的环节内容就是数据域,假如我们想从更高角度(风控、营销活动分析)去看整个业务流程并从中得到一些专题分析内容,那这里升华的部分就叫主题域。

（2）元数据规范

除了一直的词根/命名等内容，还需要保证元数据的清洗包括数据表的使用说明、存储标准、数据表负责人注明、字段类型统一等。

## 2.无用/临时数据表下线

根据数据血缘及任务依赖(这里建议在数仓侧开发血缘数据表,可不到字段血缘,如有条件可将范围扩大到可视化侧)对线上长期无用表、下游无血缘且空跑数据表、临时表进行扫描及下线,降低无用存储及计算损耗。

元数据信息采集:可使用云端数据平台自带功能如下图,如使用开源组件同学可使用三方工具Apache Ambari、Apache Atlas、等对Hive元数据信息收集,并后续存储在数据表中存放,并可以根据数据表进行重要等级打标,给予表检索及使用热度,这里建议使用DataHub开源数据中心,DataHub提供了可扩展的元数据管理平台,可以满足数据发现,数据可观察与治理。这也极大的解决了数据复杂性的问题,地址:datahubproject. io. 

## 3.应用指标公共下沉复用


由于在数仓扩张期且没有指标中心前提下大量开发应用侧数据表导致指标复用性较差问题,首先我们查看应用层指标是否口径一致,如不一致需要与下游再次沟通后修改,其次我们对应用层模型指标按照数据域、周期(1D(1天)、30D(最近30天)、60D(最近60天)、90D(最近90天)、MTD(月初至今)、TD(历史至今))拆解并将不同颗粒度下的指标放入对应数据表验证后复用,并切换线上数据表直接引用指标。

## 4.解决ODS穿透问题
依靠我们在下线无用/临时数据表时的数据血缘找到跨层引用数据表,并对这些数据表按照模型5要素(数据域、颗粒度、度量、维度、事实表类型)构建CDM(DWD与DWS)层,并验证ADS/DWS 标签/指标引用新DWD数据表的质量情况,最后完成DWD/DWS数据表上线,及DWS/ADS的引用数据表切换。

## 5.烟囱数据表重构/下线
对于线上历史多次重复开发烟囱数据表进行重构/下线,可复用公共指标以及整合其他相似场景下数据表字段内容到一个或多个数据表中,提升数据表易用性,使数据表清晰明了,由于对烟囱数据表重构/下线,从而也避免由内容不足而导致的相互依赖和任务链路延长问题发生。

## 6.元数据非合规数据表重构/修改
对原来非合规数据表元数据进行内容按照新定的标准重构,切记在建设同时需要修改下游表名依赖及代码中字段引用信息,避免线上故障发生,可以先重构ODS、DWD、DWS数据表的元数据信息,保障数据准确性后上线,后续可按照主题域分工,让组内每位同学加入进来切换ADS数据表,但在切换前需要与下游知会沟通,并拉会对切换工作计划排期,沟通后并与下游一起调整。

## 7.数据表合规后续维护企
可以从数据表价值(被引用次数、查询次数、被收藏次数等)、数据表元数据规范(按照新数据标准去检测打分)设定数据表合规评判分,并设立红黑榜,以及对应奖惩措施,后续可通过Python等开发不合规数据表信息提示(可日推、周推提醒),可通过邮件或者群信息方式,指定负责人定时治理不规范的数据表,维护好数据表的质量,同时还需要建设数据表设计中心,强制数据表上线前审核,以及按照强管控方式强行限定数据表名、词根内容、字段名等达到易读有保障效果。

## 治理后如何评估？

- 这里更多是对于数仓内部的价值，对业务的价值是提升数据使用成本降低查数找数时间
- 数据模型标准合格率，由原来xx%提升至xx%，数据模型寻找时间从x小时降低至x分钟
- 下线各层无用/临时数据表总计xxx个，释放存储资源xxxxT；
- 完成xxx个应用层烟囱数据表整合，及xxxxx公共指标下沉，建设xxxx个dws/ads公共资产；
- ODS穿透率由原来xx%下降至x%；
- 数据模型合格分数平均高于xx分
- 与团队配合完成网易某业务线数仓数据表命名、字段命名、字段类型、数据表标注规范、数据表分区生命周期等6个方向规范制定

## 数据模型治理中遇到的难点

由于在治理中后期涉及烟囱数据表重构问题,与下游多次对接无果(包含数据表迁移时需要下游配合报表改动,但只会徒增下游工作量),导致治理难推动,并且在想出合理方案时,也会因为下游各类情况导致治理进展不断Delay,后续经过多次磨合才实现数据表的更换,相互之间协调都较困难。

## 数据表合规治理思考
对于数据表合规治理是持续性的工作,数仓侧无法保障每个数据表就一定是合规的、易用的,要把数据表治理常态化,强制规范化从而减少问题发生。对于本次遇到治理困难为部门之间协调配合问题,其实在事后自己也有一个复盘思考,我觉得治理工作配合要从3个点出发：

(1)让下游配合其实最重要的是调动他们积极性,因为数据表治理对于下游来说可有可无,没有你数据表治理线上任务依旧在跑着,数仓只是修改了数据表内容,保障了易用性,可能对于下游来说毫无感知,所以可以从下游使用数据中的痛点去沟通,在优先业务支持的同时给予时间设计数据表内容,共同维护好数据标准。

(2)除了这些还可以加一些奖惩措施活动,让下游觉得配合是有价值的,例如通过红黑榜定期也给他们发送邮件或者信息,并开展简单的培训,让下游具备治理的意识,同时在他们自助治理后提供激励。

(3)如果治理在周边部门起到了效果,可以做更大的推进作用,比如我们在和下游一起做治理并取到了效果,可以发治送理效果月报/周报 发送全部门,让其他人也有感知,并定期分享自己治理心得与其他部门数据部沟通,提升数据部在公司的影响力。

# 第4期 计算资源治理

算是数仓建设实践路线-数据治理部分的补充内容

## 痛点：

- 产出产出延迟（直接影响）
- 消耗资源过多（资源打满，算不完）
- 降本增效部门预算吃紧（资源本身就少了，跟算不出来了）
- 重复开发任务/无用任务较多（也是核心资源占用问题）



## 实际项目中存在的问题

- 30+高消耗任务 ——> 大量问题代码数据倾斜，影响产出时间
- 200W+小文件 ——> API源+reduce多+本身小文件就多（Task增多+读取速度降低）
- 调度不合理 ——> 多数任务集中在2-5点CPU满载，核心/非核心任务都在争抢资源（内存空闲一般，但是CPU吃紧）
- 线上无效DQC ——> （无效DQC浪费太多资源）有没有下线的任务，并且他的DQC还在空跑+DQC资源太少导致DQC跑的时间太长
- 重复开发的任务/无用任务u ——> 资源占用/任务模型穿透率太低
- 引擎老旧/缺少调优参数mr/spark2



## 思考怎么下手

### 参数优化+引擎切换开始

补充Spark调优参数(参数内容详见文末),任务统一使用Spark3引擎加速,并充分利用Spark3的AQE特性及Z-Order排序算法特性。

AQE解释:Spark 社区在DAG Scheduler中,新增了一个API在支持提交单个Map阶段,以及在运行时修改shuffle分区数等等,而这些就是AQE,在Spark运行时,每当一个Shuffle、Map阶段进行完毕,AQE就会统这个阶段的信息,并且基于规则进行动态调整并修正还未执行的任务逻辑计算与物理计划(在条件运行的情况下),使得Spark程序在接下来的运行过程中得到优化。

- 自动倾斜处理:结合配置,aqe自动拆分reduce过大的分区,降低单个reduce task负载aqe在会收集每个分区记录数和大小 如果一个分区比中位数大就会被判定倾斜分区 随后拆解分区
- 自动合并分区:在shuffle过后,reduce task分布不齐 aqe自动合并过小的数据分区reduce task会把数据分片从map端拉回,aqe按照分区编号顺序,把小于目标尺寸的分区合并

Z-Order解释:Z-Order是一种可以将多维数据压缩到一维的技术,在时空索引以及图像方面使用较广,比如我们常用order by a,b,c会面临索引覆盖的问题,Z-Order by a,b,c效果对每个字段是对等的

### 小文件治理（详细见小文件治理部分）

略，见第1期，**（常用）**

### DQC治理

四大基础DQC：

- 空或者null
- 主键唯一
- 表行数不为空（可去）
- 表行数波动

无效DQC下线:难点在于需要查找所有DQC对应的线上任务,查看该DQC任务是否与线上任务一一匹配,从而找到无效DQC任务下线,内容繁杂耗时较多。

DQC资源:由于之前DQC配置资源为集群默认参数,效率极低导致所有DQC运行时长均超过10min,从而使得整体任务链路运行时长过久,调整Driver内存为2048M,Executor个数为2（1）,Executor内存为4096M（512MB）

### 高消耗任务治理

常见问题：数据倾斜应该怎么调优？map+reduce+参数调优

#### map端（取数，发数）

sql上的优化：

（1）行裁剪：避免`SELECT *`**（常用）**

（2）列裁剪：要加分区限定/Where过滤**（常用）**

（3）`DISTRIBUTE BY RAND()` 随机分发，分区数量一致**（常用）**

#### reduce端（算数）

数据倾斜一般发生在reduce端，数据倾斜的本质，key/数据分布不均衡

（1）distinct不要使用。使用group by

去重思路不一样：

distinct：排序取唯一，单reduce处理所有数据

group by 聚合去重：map端预聚合，reduce端预聚合（多reduce操作）

（2）尽量避免多对多关联，防止笛卡尔积（非主键join），慎用full join

where中的条件尽量下放到on中，取数阶段，就少取数，而不是join完成后再做限定

（3）map join ——> set.hive.auto.convert.join=true hive0.7后自动开启。就是小表读到内存里。

map join会把小表全部读入内存中,在map阶段直接拿另外一个表的数据和内存中表数据做匹配,由于在map是进行了join操作,省去了reduce运行的效率也会高很多map join还有一个很大的好处是能够进行不等连接的join操作,如果将不等条件写在where中,那么mapreduce过程中会进行笛卡尔积,运行效率特别低,如果使用mapjoin操作,在map的过程中就完成了不等值的join操作,效率会高很多。

（4）大key重组计算

情况一：过滤/分而治之：单独几个key比较大

- 大key都是null
  - 不想要null，直接滤掉
  - 想要null，把key=null的单独抽出来单独处理（分而治之）
- 大key不是null：分而治之（冷热key分离）
- 参数层面：`set hive.groupby.skewindata=true;`,hive在聚合操作(group by )时，监测数据倾斜性，将任务分解成更小的子任务。（现在一般用Spark3了）

情况二：加盐去盐

```sql
-- 第一步：给每条记录加盐
WITH salted_data AS (
    SELECT 
        product_id,
        CONCAT(product_id, '_', FLOOR(RAND() * 10)) as salted_id,  -- 加0-9随机盐
        column2
    FROM dwd_xxxx_order_detail_di
    WHERE ds='${lst1date}'
),

-- 第二步：按加盐后的key聚合
salted_agg AS (
    SELECT 
        salted_id,
        SUM(column2) as column2_amt
    FROM salted_data
    GROUP BY salted_id
)

-- 第三步：去盐，二次聚合
SELECT 
    SUBSTRING_INDEX(salted_id, '_', 1) as product_id,
    SUM(column2_amt) as column2_amt
FROM salted_agg
GROUP BY SUBSTRING_INDEX(salted_id, '_', 1)

-- 使用两级聚合
-- 第一级：将数据随机打散到多个桶
SELECT 
    product_id,
    FLOOR(RAND() * 10) as bucket,  -- 0-9的随机桶
    column2
FROM dwd_xxxx_order_detail_di
WHERE ds='${lst1date}'

-- 第二级：先按(product_id, bucket)聚合
-- 第三级：再按product_id聚合
```

情况三：排序问题（数据量大,求最大的1000条数据，及排名）

- 使用hash（列）%10000 + 分区内排序 ——>10000个分区
- 每一个分区取前1000条数据（避免发生极端情况，比如前1000的数据恰好在一个分区中）（这样就只有1000万的数据咯） + 全局排序（在这1000万的数据里全局排序）
- 取前1000行就可以了

```sql
-- 业务场景：
-- 再100亿数据中，对按某列，降序排列，取前1000条
SELECT id
FROM
(
	SELECT id
    	   ,ROW_NUMBER() OVER(ORDER BY id DESC) AS rn
    FROM 
    (
    	SELECT id
        	   -- 增多reduce task（增加并行度）
        	   ,ROW_NUMBER() OVER (PARTITION BY hash(id)%10000 ORDER BY id DESC) AS rn
        FROM table1
        WHERE ds='${lst1date}'
    ) t0_1
    WHERE rn <=1000
) t0
WHERE rn <= 1000
```

情况四：大表join大表**（常用）**

写一个子查询，把要查询的内容，先从大表中提取出来，尽可能裁剪列与行，然后再JOIN

 #### 参数层面

```sql
-- hive参数
set hive.auto.convert.join= true ;-- 是否自动转化成mapjoin
set hive.map.aggr=true; -- 用于控制负载均衡,顶层的聚合操作放在Map阶段执行,从而减轻清洗阶段数据传输和Reduce阶段的执行时间,提升总体性能,该设置会消耗更多的内存
set hive.groupby.skewindata=true ;-- 用于控制负载均衡,当数据出现倾斜时,如果该变量设置为true,那么Hive会自动进行负载均衡
set hive.merge.mapfiles=true ;-- 用于hive引擎合并小文件使用

-- spark参数
SET spark.sql.adaptive.enabled=true; -- 平台默认开启AQE
```

#### 任务资源分配**（常用）**

一个Excutor=4核心

- DWS，ADS字段多，计算多
  - 数据量千万以内：Drive=1024，Excutor=2，Excutor=2048/4096
  - 千万到亿：Driver=2048，Excutor=4/8；Excutor=4096/8192
- DWD，没什么计算，只有join
  - 数据量千万以内：Driver=1024，Excutor=1，Excutor=1024/2048
  - 千万到亿：Driver=2048，Excutor=2/4，Excutor=2048/4096

#### 压缩

Spark：PARQUET + snappy

快速读写:Snappy算法具有快速的压缩和解压缩速度。在读取和写入Parquet文件时,使用Snappy压缩可以提高数据的读写性能,减少I/0操作的时间开销,从而加快数据处理速度。

数据处理效率:Parquet与Snappy的组合可以提高大数据处理的效率。列式存储格式使得只需读取所需的列数据,而不必读取整个数据集,从而减少了不必要的数据读取。



### 调度安排

对于调度优化一开始会无从下手,统计凌晨2-5点区间下大概600+任务难梳理,同时存在任务依
赖,修改起来可能会对下游整体有大的影响,因此我们选择循序渐进先梳理再改善。

- 找到所有表的输出输入点即启始ODS与末尾ADS
-  划分其中核心表/非核心表,及对应任务开始时间与结束时间
-  按照梳理内容把非核心的任务穿插在当前集群资源非高峰时期(2点前与5点后),同时把核心任务调度提前,保障CDM层任务及时产出
- 对实践后内容再度调优,达到资源最大利用率



### 下线无用模型及沉淀指标到其他数据资产

烟囱表过多,需下沉指标到DWS中提升复用性,对于无用任务也需要及时下线(这里需要拿到元数据血缘最好到报表层级的数据血缘,防止任务下线后导致可视化内容问题产生),减少开发资源消耗。

### 治理后如何评估

- 这里更多是对于业务和内部价值，对外价值是减少部门费用总支出、提升任务产出速度减少业务投诉，对内价值让任务运行更快减少资源互相争抢，削峰。
- （1）Hive与Spark2任务升级Spark3.1，总计升级任务x个,升级任务后总体任务执行效率提升43%，cpu资源消耗降低41%，内存资源消耗降低46%
- （2）治理小文件数大于xxxx以上的数仓表总计xxx张，小文件总数由xxxxw下降至xxxxw
- （3）下线无效DQC总计xxxx个，修改DQC配置资源降低运行时长，由原来10min优化至3min内
- （4）完成线上xxxxx个任务优化及xxxxx个任务下线及xxx个表指标下沉，优化后节省任务耗时xxx分钟，减少CPU损耗xxxx+，降低内存消耗xxxx+（相当于节省了8个200+字段1亿数据量任务消耗）
- （5）调度重新分配后2-5点资源使用率由90+%降低至50+%，保障日用资源趋势图无大突刺波动
- （6）整体治理后为部门减少1/3总费用，由原来的xxxx万元降低至xxxx万元
- （7）数据产出平均时间由原来每日最晚9:30产出降低至7:10；

### 如何发现数据倾斜

- 定位高消耗任务，Top，平台元数据看
- 运行时长4-5小时
- 基线破线了，一般会定位数据倾斜问题

# 第5期 数据分区与链路场景优化案例分析

## 事实表分区优化设计

### 为什么需要纠结分区的设计

- 业务发展快,多过程事实数据量大
- 分层不清晰,CDM建设不完善
- 分区过大过小都会导致链路效率
- 核心线路改造升级困难,业务过度耦合



### 分区设计的原则

- 业务切分清晰
- 分区大小合适（block块，512、256、128、分区最好倍数或者相等）
- 下游扫描提速
- 主体事实可弹性操作：往往需要结合链路的设计来考虑设计的合理性
- 考量
  - 是否必要
  - 是否需要
  - 数据特别更新的选择

### 案例分析与拓展

#### 场景1：寻找合适的切分业务划分字段作为分区键

```sql
-- 已知:a 表是主表事实表,b 表是扩展表事实表,a表是系统关键表,日全量更新累计事实特性;日数据压缩后500G
-- 主要渠道明确大类,例如淘宝,拼多多,京东这些
-- 该任务为下游业务诉求任务之一

SELECT  a.s_id
		,a.s_cnt
FROM dwd_s_base_info_p a
LEFT JOIN dwd_s_base_info_ext_p b
ON a.s_id = b.s_id AND a.type IN (2, 3) AND (b.ext_name like '%渠道名%' or b.ext_name like '%渠道名%')
UNION ALL
SELECT a.s_id
	   ,a. s_cnt
FROM dwd_s_base_info_p a
WHERE a.org_name like '%渠道名%'
UNION ALL
SELECT a.s_id
	   ,a.s_cnt
FROM dwd_s_base_info a
LEFT JOIN dwd_s_base_info_ext_p b
ON a.s_id = b.s_id
WHERE system_id = 3
AND b.ext_id IN (6, 9);
```

问题：大表扫描三次，消耗太大，速度也慢；由好多任务依赖这个表，表也大，表的设计就不合理。

本来后端就应该把表拆的细一点，若短期内，后端没有解决方案，需要数据兜底。

制定一些列规则，通过UDF的方式。将规则映射到事实表中（这样这个表会多一个字段，记录规则）。然后再做成分区表，寻找合适的切分业务划分字段（规则）作为分区键，甚至可以在二级分区做分桶。关键还是要把表打细，不要扫描太多内容。

- 渠道（支付渠道、系统来源）
- 业务BU（事业部、业务BU）
- 具体场景（针对具体的产经独立拆分的分区键）

#### 场景2：历史分区的归并与归档

可以考虑把n天之前的历史数据按月分存储，也可进一步合并位年的分区。（合并 可以单独找个时间，开个任务做）

#### 场景3：配置方式手动分裂数据

没有合适的划分字段，每天全量更新，表数据量也大。

比如：

- 用“用户id”hash值，取模分桶的方式。但查数据的时候，得同样按照hash的方式去找那个桶

- 有a、b、c、d四个字段，通过一定的规则组合在一起，变成一个业务需求，然后按这个业务切分，划分分区
- 下游可以如何设计呢？可以通过视图的方式，组合不同的业务，向下游透露视图

#### 场景4：基于历史数据分裂

要做t-1分区的数据，可以专门起一个sql任务，就先基于t-2的数据，计算top50 page，按数据检索热度给t-1的数据分区，想下游展示时，用view（视图）方式展出（可以和事实表解耦）。

#### 场景5：全量分区中混合拆分（不建议用）

问题：增量业务，日增，每天25mb数据，日期一多，小文件就多。

解决：

- 一级分区
  - 存近N天的数据（比如：730天）
  - 存N天之前的数据 
- 二级分区
  - 存近N天的数据——按业务日期分区

下游在用这个表的时候不好用：要附带一个使用说明书、设计方法

基本也是全量刷

#### 场景6：分区拆分后的二次组合

把表拆成一个个小分区后，可以根据特定规则组合成业务（特定规则），然后以view视图的方式给到下游。

## 链路场景优化剖析

- 链路设计原则：分层逻辑（轻度汇总作用）
  - 链路尽可能与业务解耦，避免过多的业务定制逻辑穿透数据层
  - 链路尽可能精简高效，不要太过于冗长和杂糅
  - 各个分层数量没有明确数量限制，但也不要过度设计
- 选择一：弱化汇总的设计-部分考虑解耦——直接透出DWD
- 场景二：强化汇总-丰富汇总层——做DWS、ADS透出



# 第6期 数据质量治理

## 数据质量如何识别

- 基线/SLA破线：基线可以通过基线运维去查看每日/近一周基线破线情况,每条基线周破线次数,夜间电话告警次数,起夜次数等判断基线情况,SLA破线可从与业务方对接,对数仓产出投诉去查看。
- DQC及任务失败频出：可根据DQC频发次数，任务失败次数等情况去判断，或某个任务导致经常起夜查看，这里DQC频发不一定是DQC有问题，很大概率都是数据波动，数据分布略高/低导致
- 数据质量问题频出：通过下游在问题上报平台、需求平台（数据质量问题记录）、共享excel等找到共通性频发问题去统一治理，或者查看其他数据表是否存在此类情况，对于数据源头问题需要把问题放入长期跟踪检测体系中去运维 可以从数据质量问题出发，找到任务经常出现数据质量问题对应的数仓同学，指定奖惩及培训措施降低问题发生

## 数据质量治理流程

### 基线告警问题拆解 

#### 产出延迟任务

确定破线/预警基线,通过数据表采集的元数据信息表通过数据血缘链路串起来,通过任务元数据找到告警基线任务中实际结束时间减开始运行最长的,即依赖部分晚产出(1-3小时运行时长)数据导致基线破线(包括倾斜任务、算法任务等),需要对这块任务调整或依赖t-2降低时效保障产出。

解决方案：

- 上游没有产出：数据质量长期监测体系（数仓建设实践路线——数据质量）
- 自己没有产出
  - 去看SparkUI数据倾斜的情况
  - eazydata数据360看任务耗时排名（定位任务）
  - 必要的时候，使用t-2降低时效性，保障产出（跨部门情况）

#### 链路过长导致产出延迟

找到所有表的输出输入点即启始ODS表(接入层)与末尾ADS表(应用层),ADS回溯到DWD(明细层)或ODS向下游依赖找问题,可能存在指标反复加工计算或者数据表反复依赖(ADS依赖ADS再依赖ADS导致)找到依赖问题原因,这里多数都是ADS指标反复依赖导致,需要对指标进行公共沉淀到DWS(轻度指标汇总),形成公共指标复用,或建设ADS核心指标/标签宽表,减少重复依赖问题。

解决方案：

- 根据数据血缘去找任务/表的一个依赖关系
- 指标复用
  - ADS层进行公共指标下沉，到DWS
  - 建立ADS宽表

#### 高优先级基线调度时间重叠

通过任务元数据梳理同时间运行的节点任务,根据当前资源消耗波动,把重叠基线调度时间提前,进行削峰填平,充分使用其他闲置时间资源。

(1)在总体的资源占用时间图上,资源打满/占用多的时间段

(2)在对应时间段下,按照破线的基线找对应的任务

(3)线下跑一下任务,对比线上任务运行的事件,如果差异比较大,就是发生资源争抢

(4)重新配置调度,错峰执行任务,基线也错开/升降基线优先级

#### 任务失败导致告警

多半是DQC挂掉了

- 字段没对上
- OOM问题

解决方案：

- 阈值可以设置大一点，一些简单的问题就不要强制停止了（DQC层面）
- 模型审核可以严格一点，不要出现简单问题，轻易出发DQC（代码审核要严格）

#### 值班手册：找问题、找人

<img src=".\images\数据质量治理_1.png" style="zoom:67%;" />

<img src=".\images\数据质量治理_2.png" style="zoom:50%;" />

<img src=".\images\数据质量治理_3.png" style="zoom:67%;" />

### DQC治理

- 基础DQC：有包含关系的可以合并一下,表行数波动率包含,表不为空(-5%,30%)波动率可以用15日的线性回归来做估算(Dataworks功能)
- 业务DQC：不用的下线 + 常报警的DQC上下游流程和表看一下，去调整

### 数据开发/校验流程重制定

(在需求确定的时候,需要对需求进行评估,业务方要阐明需求的价值,然后再考虑做不做)

- 模型审核:5要素+维度退化/脱敏+表列名规范+任务owner与注释
- 数据校验:
  - 数据探查+数据对比
    - 探查是看数据分布情况，大小值等
    - 数据比对是，更改字段或者重做后的表与之前数据是否有区别
  - 抽样检测=>ODS抽样数据,计算到ADS,和线上ADS做比较
    - 采用抽样数据写从ODS取出来逻辑与自己开发的抽样指标数据比对保障开发内容能对齐,从ODS取数写SQL去关联清洗,通过算子加工与最后呈现指标去比对,但这里数据只能完成80%准确度,剩余20%需要与数据分析同学进行联调。

## 数据质量治理后评估

- 这里更多是对于业务和内部价值，对外价值是数据问题更少发生，任务更稳定
- （1）通过对基线/SLA治理，夜间值班培训及手册，保障核心任务按时产出，原每周3天不能准时产出降低至小于等于1天；
- （2）对DQC及任务治理系统化治理，自动化配置，保障夜间值班同学起夜率降低至xx%
- （3）通过对原有业务方提来的bug找到共通性问题，同时加强数仓团队数据质量理念，制定奖惩措施，数仓侧原有bug从每月40-50个，降低至每月小于7个

## 数据质量治理思考

借助AI：实现智能化治理 ——>如问题任务识别 + 自动治理

数据质量治理核心在于稳定数据交付,降低组内运维情况,更多在于业务数据资产扩张后遗留下来问题,除了如上保障手段,后续可借助AI对线上任务运行进行全面诊断、基线诊断,辅助数仓同学快速定位当前错综复杂问题。

# 第7期 存储资源治理

## 背景

由于早期数仓在存储资源充足情况下，未考虑到后续扩容和存储格式问题导致后续存储资源紧张，从而需要整体治理

治理时间点：早期一般不做，一般到达存储水位线（70%）/降本增效的时候才会做

## 存储治理总体思路

梳理出长期未被使用/引用模型,及生命周期不符合当前标准数据模型,未分区,空表,文件数,文件格式,长期全量存储等**(通过元数据模型或平台捞出)**。

优化方案,可以从这几点优先入手。用最少的人力成本和时间成本,达到显著的优化。

- 下线无用数据表节省存储->
- 存储格式及压缩格式配置->
- 分区生命周期优化->
- 根据业务情景实现节省存储即数据模型优化,这里去除了小文件治理,感兴趣同学可以看如上治理课程。

## 存储资源问题识别及治理

### 0.获取存储资源元数据及进行数据血缘元数据

通过采集的元数据信息,进行元数据表梳理及建设(元数据来源可以通过开源工具、数据平台自带元数据、工具二次开发等方式进行获取,这里展示是数据平台进行元数据采集)

- 多表之间的数据血缘

![](.\images\存储治理_1.png)

- 各单表的元数据信息

![](.\images\存储治理_2.png)

### 1.无用/临时数据表下线评估

在有了元数据表之后，就可以开始治理

- 临时表治理:
  - 去元数据信息表中搜索临时表
  - 结合数据血缘,如果临时表上下游该临时表没有血缘依赖,可以删除
- 无用表治理:去A2表中找无用表
  - 看表的引用次数/搜索次数=>是否无用表
- 空表:无血缘,无数据

### 2.存储格式+压缩

- spark3 => Parquet + snappy
- 使用Parquet作为存储格式,sparksql可以更有效的进行调度和测试
  - 优化执行路径
  - 减少stage执行的消耗
  - 减低CPU消耗
  - 下推过滤器,减少磁盘IO和内存占用
- snappy=>快速压缩和解压缩,CPU资源消耗少
- gzip=>高压缩率,解压速度慢,CPU占用大,适合归档大量传输数据,节省带宽和存储占用
- 支持大量小文件任务的集体整治
  - 如果历史表，有用text格式存储，可以用Spark3跑动态分区，历史回刷（创建一个新表，用Parquet格式，数据还是原来的数据，然后替换掉原来的表）

### 3.分区生命周期调整

通过数据表分区生命周期字段去查看,1要确认表分区是否永久,2要确认表分区范围是否合理,因此需要结合当前现状重新指标表分区(分区标准只是概念,通用数据表可参考,但对于di表及部分ads应用表需要因地制宜)

<img src=".\images\存储治理_3.png" style="zoom:50%;" />

### 4.分区优化及增全量修改

- 分区细化：可以把二级分区变成三级分区，二级分区用业务域来划分

对于分区优化,如果只有日期分区可以直接进行分区裁剪如第四点讲到的内容,这里特指二级分区,二级分区例如我们之前课程中讲到的日期+场景、日期+规则,由于二级分区+日期不断增多导致数据查询过慢,分区量暴涨,导致数据模型极为难用,甚至部分表分区超过3w,对于这种情况1.直接拆表按照二级分区内容再做归类。

举例按照之前金融风险名单规则来看,目前二级分区包括最大预期天数、失信人、坏账、多头坏账等等规则,目前规则数量达到500+,所以每天生成分区则是500个,对于这种情况可以按照规则进行拆解,例如金融风险名单-金融负面、金融风险名单-司法负面、金融风险名单-企业负面等等,拆解规则提升模型易用能力,同时也降低了耦合(例如某个规则无法产出)。

- 全量改增量

  - 历史分区不动，在一个分区里
  - 增量一个分区（比如存7天的增量，然后合并到全量中（7天一合并）

  增全量分区,可以从订单或其他视角来看,如之前存储采用全量分区存放,虽然提升便捷但分区不能这么支撑,所以需要全改增,对于历史完成的订单我们放到2099-01-01分区形成关单,对于t-1分区则通过create_time及update_time获取t-1对应新增或修改数据存放。

  具体可以看数据技术——8期，全量改增量

## 存储治理后评估

- 这里更多是对于业务和内部价值，对外价值是减少部门费用总支出
- （1）下线各层无用/临时数据表总计xxx个，释放存储资源xxxxT；
- （2）使用Parquet格式+Snappy压缩，提升压缩比，存储资源由原来xxxxT降低至xxxxT
- （3）统一生命周期节省不必要的存储资源，对于临时表采用7天表存储生命周期，对于每一分层、业务进行统一裁剪，存储资源由原来xxxxT降低至xxxxT
- （4）根据不同业务场景，通过拉链表，增全力量方式存储存储资源由原来xxxxT降低至xxxxT
- （5）整体治理后为部门减少1/3总费用，由原来的xxxx万元降低至xxxx万元

## 监控打分

![](.\images\存储治理_4.png)



# 第8期 报表治理

## 报表治理背景

大家在开发中会遇到这样一种情况,我们的日常任务/数据表都是不断上线的,但持续上线会导致存储、计算资源飙升,导致部门每年资源预算都不充足,因此每年给大老板报预算时候预算都在增长,同时内部数据表、报表想下线时,由于不清楚有没有被使用,都被阻拦,因此产生恶心循环,目前线上报表大多数都是一次性的,可能是为了某个项目的业务侧阶段汇报、大老板想简单看个数据分布从而制定决策,因此对于无效报表、无效任务、无效数据表都可以进行治理优化,同时也需要对报表进行加速查询从而提效。

## 报表治理思考（治理前）

由于线上报表过多,同时很多报表业务方都在使用,如果直接上手则会导致业务投诉,因此需要先

排查哪些报表还有存在价值->从而再去定位对应数据表、任务->下线后看到收益->梳理出核心报表再放入数据门户中。

## 报表梳理

在期初先梳理哪些任务下游有报表在使用,在网易easy data可以通过查看下游影响对象从而知道哪些数据表下游有报表进行捞取,可以直接通过元数据捞取。

如没有打通报表和数据源之间的血缘也可以手动去统计报表梳理报表使用方、核心等级、使用的数据表、最近7天访问等。

| 报表名称         | 报表用途                                       | 报表使用方         | 使用的数据表             | 最近7天报表访问量 | 报表截图 |
| ---------------- | ---------------------------------------------- | ------------------ | ------------------------ | ----------------- | -------- |
| 商品营销数据大盘 | 用于查看各类商品有效情况，方便运营活动策略分析 | 商品运营、数据分析 | ads_xx_xx<br />ads_xx_xx | 13                |          |

## 报表确认

根据梳理使用情况后,可以根据7日访问小于等于2的报表进行治理,在治理之前还需与业务方进行沟通,如业务方觉得这个报表他做不了主则可以上升leader去问,如最终决定保留报表(需要给出理由)则可以约定不下线报表,下线任务缩短存储分区生命周期方便历史全量回溯。

## 报表治理流程（治理中）

### 报表下线

这里报表下线要区别是需要留存的报表还是可以直接下线的报表,这里有一定区别,可以直接下线的报表是跟业务方确认后完全没人使用及近7天访问量跌0的报表,可以直接点击报表下线,至于需不需要删除则看当前空间内部报表数量,例如空间存放的报表较少资源不足则可以。

### 报表对应数据表及任务下线/冻结

在确定要下线报表后找到对应ads表、ads任务，对于直接下线的报表可以直接下线任务（下线的任务一定是没有下游血缘的），对于数据表可删除，但下线的任务记得保留（就是代码），可以备注已废弃，就怕未来可能要回滚。

对于不确定保留的报表可以保留报表，下线任务，缩短数据表生命周期例如保留7天，如果是核心任务场景报表也不清楚要不要下线可以不下线报表、缩短生命周期，取消基线配置，将任务时间定到午休时，这样不会影响线上其他任务使用资源还可以保障报表有数据。

### 报表资源治理

对于线上运行的报表有时运行很慢也会被业务方反馈，因此报表除了有效之外还需要查询高效

（1）对于报表首先需要看报表运行时间，网易有数具备报表运行检测功能，可检测出报表查询耗时方便观测，同时跑数据集时也有查询时间也可以通过此评测。(批判运行时间)

（2）对于未使用olap查询也需要改造，将原来通过maxcompute、spark运行改成olap加速（阿里云adb、star rocks），将ads表传输至olap中即可。（切换到OLAP，加快读取运算）

（3）预计算，语兴发现很多报表跑不动原因在于使用大量明细数据，都通过dwd直接连接，并在报表上去做指标，这样也会大大降低查询效率，可以先将指标计算好后在报表上展示，但对于使用多维度筛选尤其是时间维度时则需要慎重，例如审核数据报表，里面存在发布时间、审核时间（时间还会通过每日快照去变化），xxx时间等，如果通过预计算去跑则指标会被定死，导致不能灵活查询，因此对于这种数据只能限制map端，通过少读数据（限制时间+其他字段）减少后续计算压力，至于要读取dwd最近多少天的数据则可以与业务方商量。

同时对于5min、30min数据就不要走离线任务了，通过flink将数据落olap，在报表聚合当日数据，对于历史数据还读离线表即可，最终在数据集进行union all

```sql
SELECT xxx
	   ,xxx
FROM dwd_xxx_xxx_ri -- 实时数据

UNION ALL

SELECT xxx
	   ,xxx
FROM dwd_xxx_xxx_df -- 离线数据
```

### 报表权限治理

报表权限隔离尤为重要，语兴建议这里按照空间隔离更好，例如基础中台一个空间、社区一个空间、金融一个空间，同时不同账号需要设置到对应空间权限，同时同空间报表也需要隔离。

但权限治理其实在语兴看来是挺鸡肋的一件事，如果是买三方服务例如阿里quick bi、网易有数都是有账号数量的，如果选择不对外展示那就要准备足够多的账号，最后会形成多个人用一个阿里云账号情况（这种情况在业务侧使用时遇到了多次），甚至还会相互借用，防了，但不一定防得住。

## 报表维护（治理后）

在治理后可以将核心报表维护到数据门户中按照主题域进行分类管理形成一站式管理，解决了业务方找不到核心报表问题。

同时报表治理也是持续性治理，后续可通过python代码将每周访问量低的报表推送给对应的负责人进行提醒，确认治理给予反馈及督促下线处理，当然治理还是长期的事，对于报表治理可以1年做一次清理打扫。

## 治理价值

(1)释放无用ads数据表、缩短数据表生命周期采用标准tt总计40+,存储资源xxT,下线无用报表对应数据任务30+,降低夜间任务并行;

(2)释放无人使用报表资源20+,打造数据门户,完成近80+报表维护;

(3)通过预计算、olap切换、map侧限制实现30+报表平均返回时间从38s下降至10s内;





























